* x-to-openai-api-adapter

This is a Cloudflare worker that acts as an adapter to proxy requests to the
[[https://platform.openai.com/docs/guides/gpt/chat-completions-api][OpenAI Chat Completions API]] to other AI services like Claude, Azure OpenAI, and
Google Palm.

This project draws inspiration from, and is based on, the work of
[[https://github.com/haibbo/cf-openai-azure-proxy][haibbo/cf-openai-azure-proxy]]. If you only require a single service proxy (such
as Claude/Azure), please consider using the original project. This project,
however, is more complex and is specifically designed to accommodate multiple
services.

#+begin_ai markdown :model gpt-4 :temperature 0.5
[SYS]: You're a native en-us speaker who is also an expert in Emacs Org mode.

[ME]: Grammar fix and polish the following text in Org mode:
This project is inspired and based on the work of [[https://github.com/haibbo/cf-openai-azure-proxy][haibbo/cf-openai-azure-proxy]],
If you just need a single service proxy(Claude/Azure), just use the original project. This project is more complex and designed for multiple services.


[AI]: This project draws inspiration from, and is based on, the work of [[https://github.com/haibbo/cf-openai-azure-proxy][haibbo/cf-openai-azure-proxy]]. If you only require a single service proxy (such as Claude/Azure), please consider using the original project. This project, however, is more complex and is specifically designed to accommodate multiple services.

[ME]:
#+end_ai

** Overview

The worker allows you to make requests to the OpenAI API format and seamlessly
redirect them under the hood to other services. This provides a unified API
interface and abstraction layer across multiple AI providers.

*** Key Features

- *Single endpoint to access multiple AI services*;
- Chat to *Multiple models* in a single request;
- Unified request/response format using the OpenAI API;
- Auto handles streaming responses;
- Single API key for authentication;
- Support for OpenAI, Claude, Azure OpenAI, and Google Palm;


** Usages

To use the adapter, simply make requests to the worker endpoint with the OpenAI
JSON request payload.

Behind the scenes the worker will:

- Route requests to the appropriate backend based on the `model` specified
- Transform request payload to the destination API format
- Proxy the request and response
- Convert responses back to OpenAI format


*** Request Example

For example, to use =gpt-3.5-turbo=:

#+begin_src json :exports both
{
	"model": "gpt-3.5-turbo",
	"stream": true,
	"messages": [
		{
			"role": "user",
			"content": "Hello there!"
		}
	]
}
#+end_src

To use =claude-2=:

#+begin_src json :exports both
{
	"model": "claude-2",
	"stream": true,
	"messages": [...]
}
#+end_src


You can specify *multiple models* (delimitered by ~,~) to query in parallel:

#+begin_src json :exports both
{
	"model": "gpt-3.5-turbo,claude-2",
	"stream": true,
	"messages": [...]
}
#+end_src

The response will contain the concatenated output from both models streamed in
the OpenAI API format.

Other OpenAI parameters like `temperature`, `stream`, `stop` etc. can also be
specified normally.

*** Python Example

#+begin_src python :exports both :results output
import openai

openai.api_key = "<your specified API_KEY>"
openai.api_base = "<your worker endpoint>/v1"

# For example, the local wrangler development endpoint
# openai.api_key = 'sk-fakekey'
# openai.api_base = "http://127.0.0.1:8787/v1"

chat_completion = openai.ChatCompletion.create(
    model="gpt-4,claude-2",
    messages=[
        {
            "role": "user",
            "content": "A brief introduction about yourself and say hello!",
        }
    ],
    stream=True,
)


for chunk in chat_completion:
    if chunk["choices"]:
        print(chunk["model"], chunk["choices"][0]["delta"].get("content", ""))
#+end_src

** The /X/ (API Services supported)

*** [X] OpenAI
CLOSED: [2023-07-18 Tue 21:08]
*** [X] Azure OpenAI
CLOSED: [2023-07-18 Tue 21:09]
*** [X] Claude
CLOSED: [2023-07-18 Tue 21:09]
*** [X] Google Palm
CLOSED: [2023-07-18 Tue 21:09]

** The /models/ suported

Here are the models currently supported by the adapter service:

To use a particular model, specify its ID in the `model` field of the request body.

*** OpenAI Models:

- gpt-3.5-turbo
- gpt-3.5-turbo-0613
- gpt-3.5-turbo-16k
- gpt-3.5-turbo-16k-0613
- gpt-4
- gpt-4-0613

*** Azure OpenAI Models(Depending on your deployment name):

/For example, the following models are the deployment names from my azure openai service./

You'll have to update the [[file:./src/models.ts]] for your own deployment names.

- gpt-35-turbo
- gpt-35-turbo-16k

*** Claude Models:

- claude-instant-1(claude-instant-1.1)
- claude-2(claude-2.0)

*** Google Palm Models:

- text-bison-001
- chat-bison-001


** Deployment


[[https://deploy.workers.cloudflare.com/?url=https://github.com/lroolle/x-to-openai-api-adapter][Deploy to Cloudflare Workers]]


To deploy, you will need:

- Cloudflare account
- API keys for each service

*** Install wrangler

#+begin_src sh :exports both :wrap src sh :results raw replace
npm i wrangler -g
#+end_src

*** Environment Variables
Configure the worker environment variables with your secret keys.

#+begin_src sh :exports both :wrap src sh :results raw replace
wrangler secret put API_KEY
wrangler secret put OPENAI_API_KEY
wrangler secret put AZURE_OPENAI_API_KEY
wrangler secret put ANTHROPIC_API_KEY
wrangler secret put PALM_API_KEY
#+end_src

Or you can add the keys after deploy using the Cloudflare dashboard.

#+begin_quote
Worker -> Settings -> Variables -> Environment Variables
#+end_quote


*** Run publish/deploy

#+begin_src sh :exports both :wrap src sh :results raw replace
wrangler depoly
#+end_src

** TODO List

*** [ ] Fix handle claude stream
- Bugs in handle claude stream that it can not iterate the whole stream, sometimes.
*** [ ] Handle multiple models merge JSON?
:LOGBOOK:
- State "[ ]"        from              [2023-07-18 Tue 21:10]
:END:
*** [ ] Error handling
:LOGBOOK:
- State "[ ]"        from              [2023-07-18 Tue 21:17]
:END:
*** [ ] Refactor the proxy.ts to different files
:LOGBOOK:
- State "[ ]"        from              [2023-07-18 Tue 21:25]
:END:
*** [ ] A generic way to configure the Azure OpenAI models
:LOGBOOK:
- State "[ ]"        from              [2023-07-18 Tue 21:22]
:END:

*** [ ] Mutiple API_KEYs support
:LOGBOOK:
- State "[ ]"        from              [2023-07-18 Tue 21:09]
:END:
- Config multiple API_KEYs
- Maybe specify different key by the model?
- Maybe retriable for the same model with different keys?
*** [ ] A basic metric API/dashboard
:LOGBOOK:
- State "[ ]"        from              [2023-07-18 Tue 21:10]
:END:


** Development

Create a ~.dev.vars~ with your environment API_KEYs, then run:

#+begin_src sh :exports both :wrap src sh :results raw replace
wrangler dev
#+end_src


#+begin_src sh :exports both :wrap src sh :results raw replace
curl -vvv http://127.0.0.1:8787/v1/chat/completions -H "Content-Type: application/json" -H "Authorization: Bearer sk-fakekey" -d '{
    "model": "gpt-3.5-turbo,claude-2", "stream": true,
    "messages": [{"role": "user", "content": "say hello world, no other words or punctuation please."}]
  }'

#+end_src



** Contributions

Contributions and improvements are welcome! Please open GitHub issues or PRs.

Let me know if you would like any changes or have additional sections to add!
